{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c6175afe",
   "metadata": {},
   "source": [
    "## Norfair Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b5ef8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*-coding:utf-8 -*-\n",
    "\"\"\"\n",
    "  ████\n",
    "██    ██   Datature\n",
    "  ██  ██   Powering Breakthrough AI\n",
    "    ██\n",
    " \n",
    "@File    :   Norfair.ipynb\n",
    "@Author  :   Keechin Goh\n",
    "@Version :   1.0\n",
    "@Contact :   hello@datature.io\n",
    "@License :   Apache License 2.0\n",
    "@Desc    :   Norfair Tracking Script\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b662cec",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "This jupyter notebook runs a Norfair Algorithm atop of various object detection (bounding box) models to track objects in video files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd91dc5",
   "metadata": {},
   "source": [
    "## Install Python Dependencies\n",
    "\n",
    "Python Version Used: 3.7 =< version =< 3.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe4a404",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U tensorflow==2.5.0\n",
    "!pip install -U numpy==1.18.5\n",
    "!pip install -U Pillow==7.2.0\n",
    "!pip install -U opencv-python==4.5.1.48\n",
    "!pip install -U norfair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d81c47",
   "metadata": {},
   "source": [
    "## Import Python Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68d01ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import norfair\n",
    "import os.path\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import Image\n",
    "from norfair import Detection, Tracker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6388c29a",
   "metadata": {},
   "source": [
    "## Load Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a998d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_label(model_path):\n",
    "    \"\"\"Reads label map in the format of .pbtxt and parse into dictionary\n",
    "\n",
    "    Args:\n",
    "        label_map_path: the file path to the label_map\n",
    "\n",
    "    Returns:\n",
    "        dictionary with the format of {label_index: {'id': label_index, 'name': label_name}}\n",
    "    \"\"\"\n",
    "    label_map_path = os.path.join(model_path, \"label_map.pbtxt\")\n",
    "\n",
    "    if os.path.exists(label_map_path) is False:\n",
    "        raise FileNotFoundError(\"No valid label map found.\")\n",
    "\n",
    "    label_map = {}\n",
    "\n",
    "    with open(label_map_path, \"r\") as label_file:\n",
    "        for line in label_file:\n",
    "            if \"id\" in line:\n",
    "                label_index = int(line.split(\":\")[-1])\n",
    "                label_name = next(label_file).split(\":\")[-1].strip().strip(\"'\")\n",
    "                label_map[label_index] = {\"id\": label_index, \"name\": label_name}\n",
    "\n",
    "    return label_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee960ecc",
   "metadata": {},
   "source": [
    "## Load Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7c12f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(img, height, width):\n",
    "    \"\"\"Load an image from base64 into a numpy array.\n",
    "\n",
    "    Puts image into numpy array to feed into tensorflow graph.\n",
    "    Note that by convention we put it into a numpy array with shape\n",
    "    (height, width, channels), where channels=3 for RGB.\n",
    "\n",
    "    Args:\n",
    "        image_base64: image in base64 format\n",
    "        height: height of image\n",
    "        width: width of image\n",
    "\n",
    "    Returns:\n",
    "        uint8 numpy array with shape (img_height, img_width, 3)\n",
    "    \"\"\"\n",
    "    image_shape = np.asarray(img).shape\n",
    "\n",
    "    image_resized = img.resize((height, width))\n",
    "    return np.array(image_resized), (image_shape[0], image_shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315a706a",
   "metadata": {},
   "source": [
    "## Prediction Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0a37e4",
   "metadata": {},
   "source": [
    "### Bounding Box Tensorflow Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d0993f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_tf_predition(img, trained_model, size, threshold):\n",
    "    \"\"\"Run prediction on image in base64 format with trained tf bounding box model and label.\n",
    "\n",
    "    Args:\n",
    "        img: input image\n",
    "        trained_model: loaded tensorflow model.\n",
    "        size: size of image to load.\n",
    "        threshold: confidence threshold.\n",
    "\n",
    "    Returns:\n",
    "        norfair Detection type\n",
    "    \"\"\"\n",
    "\n",
    "    ## Run prediction\n",
    "    height, width = size.split(\",\")\n",
    "\n",
    "    ## Returned original_shape is in the format of width, height\n",
    "    image_resized, origi_shape = load_image_into_numpy_array(\n",
    "        img, int(height), int(width)\n",
    "    )\n",
    "\n",
    "    ## The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
    "    input_tensor = tf.convert_to_tensor(image_resized)\n",
    "\n",
    "    ## The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
    "    input_tensor = input_tensor[tf.newaxis, ...]\n",
    "\n",
    "    ## Feed image into model\n",
    "    detections_output = trained_model(input_tensor)\n",
    "\n",
    "    num_detections = int(detections_output.pop(\"num_detections\"))\n",
    "    detections = {\n",
    "        key: value[0, :num_detections].numpy()\n",
    "        for key, value in detections_output.items()\n",
    "    }\n",
    "    detections[\"num_detections\"] = num_detections\n",
    "\n",
    "    ## Filter out predictions below threshold\n",
    "    indexes = np.where(detections[\"detection_scores\"] > float(threshold))\n",
    "\n",
    "    ## Extract predictions\n",
    "    bboxes = detections[\"detection_boxes\"][indexes]\n",
    "    classes = detections[\"detection_classes\"][indexes].astype(np.int64)\n",
    "    scores = detections[\"detection_scores\"][indexes]\n",
    "    detection_info = {\n",
    "        \"boxes\": bboxes.tolist(),\n",
    "        \"classes\": classes.tolist(),\n",
    "        \"scores\": scores.tolist(),\n",
    "    }\n",
    "    if len(detection_info[\"boxes\"]) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        detection_out = []\n",
    "        for i in range(0, len(detection_info[\"boxes\"])):\n",
    "            temp_box = [[0, 0], [0, 0]]\n",
    "            temp_box[0][0] = detection_info[\"boxes\"][i][1] * origi_shape[1]\n",
    "            temp_box[0][1] = detection_info[\"boxes\"][i][0] * origi_shape[0]\n",
    "            temp_box[1][0] = detection_info[\"boxes\"][i][3] * origi_shape[1]\n",
    "            temp_box[1][1] = detection_info[\"boxes\"][i][2] * origi_shape[0]\n",
    "\n",
    "            detection_out.append(\n",
    "                Detection(\n",
    "                    points=np.array(temp_box),\n",
    "                    scores=np.array(\n",
    "                        [detection_info[\"scores\"][i], detection_info[\"scores\"][i]]\n",
    "                    ),\n",
    "                    label=int(detection_info[\"classes\"][i]),\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return detection_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a57ae95",
   "metadata": {},
   "source": [
    "## Norfair Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a420de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_distance(detected_pose, tracked_pose):\n",
    "    detection_centroid = np.sum(detected_pose.points, axis=0) / len(detected_pose.points)\n",
    "    tracked_centroid = np.sum(tracked_pose.estimate, axis=0) / len(detected_pose.points)\n",
    "    distances = np.linalg.norm(detection_centroid - tracked_centroid, axis=0)\n",
    "    return distances / (KEYPOINT_DIST_THRESHOLD + distances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ae0a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Norfair_tracking(\n",
    "    video_path,\n",
    "    model_path,\n",
    "    size,\n",
    "    threshold,\n",
    "    output_format,\n",
    "    output_vid_trk_path,\n",
    "    COLORS,\n",
    "    DISTANCE_THRESHOLD,\n",
    "    DETECTION_THRESHOLD,\n",
    "    HIT_COUNTER_MAX,\n",
    "    INITIALIZATION_DELAY,\n",
    "    POINTWISE_HIT_COUNTER_MAX,\n",
    "):\n",
    "    global KEYPOINT_DIST_THRESHOLD\n",
    "    # load model\n",
    "    trained_model = tf.saved_model.load(os.path.join(model_path, \"saved_model\"))\n",
    "    # load map\n",
    "    category_index = load_label(model_path)\n",
    "\n",
    "    # read in video\n",
    "    vid = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # initialize video writer\n",
    "    width = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(vid.get(cv2.CAP_PROP_FPS))\n",
    "    codec = cv2.VideoWriter_fourcc(*output_format)\n",
    "    out = cv2.VideoWriter(output_vid_trk_path, codec, fps, (width, height))\n",
    "\n",
    "    # initialize tracker\n",
    "\n",
    "    tracker = Tracker(\n",
    "        distance_function=bbox_distance,\n",
    "        distance_threshold=DISTANCE_THRESHOLD,\n",
    "        detection_threshold=DETECTION_THRESHOLD,\n",
    "        hit_counter_max=HIT_COUNTER_MAX,\n",
    "        initialization_delay=INITIALIZATION_DELAY,\n",
    "        pointwise_hit_counter_max=POINTWISE_HIT_COUNTER_MAX,\n",
    "    )\n",
    "    KEYPOINT_DIST_THRESHOLD = height / 40\n",
    "\n",
    "    idx = 0\n",
    "    track_count = [0] * 1000000\n",
    "    while True:\n",
    "        # Read Video\n",
    "        (grabbed, frame) = vid.read()\n",
    "        if not grabbed:\n",
    "            break\n",
    "\n",
    "        # Do detection on every video frame\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        prediction = bbox_tf_predition(\n",
    "            Image.fromarray(frame), trained_model, size, threshold\n",
    "        )\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        if prediction == None:\n",
    "            out.write(frame)\n",
    "            print(\"Frame \" + str(idx) + \" is written!\")\n",
    "            idx += 1\n",
    "            continue\n",
    "\n",
    "        tracks = tracker.update(detections=prediction)\n",
    "        for track in tracks:\n",
    "            if track_count[int(track.id)] > track.hit_counter:\n",
    "                track_count[int(track.id)] = track.hit_counter\n",
    "                continue\n",
    "            track_count[int(track.id)] = track.hit_counter\n",
    "            bbox = [0, 0, 0, 0]\n",
    "            bbox[0] = track.last_detection.points[0][0]\n",
    "            bbox[1] = track.last_detection.points[0][1]\n",
    "            bbox[2] = track.last_detection.points[1][0]\n",
    "            bbox[3] = track.last_detection.points[1][1]\n",
    "            class_name = category_index[int(track.label)][\"name\"].strip('\"')\n",
    "            color = tuple(int(c) for c in COLORS[int(track.id) % len(COLORS)])\n",
    "\n",
    "            # draw bbox on screen\n",
    "            cv2.rectangle(\n",
    "                frame,\n",
    "                (\n",
    "                    int(bbox[0]),\n",
    "                    int(bbox[1]),\n",
    "                ),\n",
    "                (\n",
    "                    int(bbox[2]),\n",
    "                    int(bbox[3]),\n",
    "                ),\n",
    "                color,\n",
    "                int((width + height) / 600),\n",
    "            )\n",
    "            ## Draw label background\n",
    "            cv2.rectangle(\n",
    "                frame,\n",
    "                (\n",
    "                    int(bbox[0]),\n",
    "                    int(bbox[3]),\n",
    "                ),\n",
    "                (\n",
    "                    int(bbox[2]),\n",
    "                    int(bbox[3] + int((width + height) / 108)),\n",
    "                ),\n",
    "                color,\n",
    "                -1,\n",
    "            )\n",
    "            cv2.putText(\n",
    "                frame,\n",
    "                class_name,\n",
    "                (int(bbox[0]), int(bbox[3] + int((width + height) / 300))),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                (width + height) / 7500,\n",
    "                (0, 0, 0),\n",
    "                int((width + height) / 3000),\n",
    "                cv2.LINE_AA,\n",
    "            )\n",
    "            cv2.putText(\n",
    "                frame,\n",
    "                \"ID: \" + str(track.id),\n",
    "                (int(bbox[0]), int(bbox[3] + int((width + height) / 136))),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                (width + height) / 7500,\n",
    "                (0, 0, 0),\n",
    "                int((width + height) / 3000),\n",
    "                cv2.LINE_AA,\n",
    "            )\n",
    "\n",
    "        result = np.asarray(frame)\n",
    "        result = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        # save video\n",
    "        out.write(frame)\n",
    "        print(\"Frame \" + str(idx) + \" is written!\")\n",
    "        idx += 1\n",
    "\n",
    "    vid.release()\n",
    "    out.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcd3ef5",
   "metadata": {},
   "source": [
    "## Run Norfair for Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616de082",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "video_path = \"./sample_video.mp4\"\n",
    "model_path = \"./model\"\n",
    "output_vid_trk_path = \"./output_\" + video_path.split(\"./\")[1].split(\".\")[0] + \".mp4\"\n",
    "size = \"320,320\"\n",
    "threshold = \"0.7\"\n",
    "output_format = \"mp4v\"\n",
    "COLORS = np.random.randint(0, 255, size=(200, 3))\n",
    "\n",
    "# Define constants\n",
    "DETECTION_THRESHOLD = 0.6\n",
    "HIT_COUNTER_MAX = 45\n",
    "INITIALIZATION_DELAY = 4\n",
    "POINTWISE_HIT_COUNTER_MAX = 10\n",
    "DISTANCE_THRESHOLD = 0.8\n",
    "\n",
    "start = time.time()\n",
    "output_vid = Norfair_tracking(\n",
    "    video_path,\n",
    "    model_path,\n",
    "    size,\n",
    "    threshold,\n",
    "    output_format,\n",
    "    output_vid_trk_path,\n",
    "    COLORS,\n",
    "    DISTANCE_THRESHOLD,\n",
    "    DETECTION_THRESHOLD,\n",
    "    HIT_COUNTER_MAX,\n",
    "    INITIALIZATION_DELAY,\n",
    "    POINTWISE_HIT_COUNTER_MAX\n",
    ")\n",
    "end = time.time()\n",
    "print(\n",
    "    \"Tracking by Norfair for \" + video_path + \" takes \" + str(int(end - start)) + \"s.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab6614c",
   "metadata": {},
   "source": [
    "## Display Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f77795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "\n",
    "def display_video(path):\n",
    "    mp4 = open(path, \"rb\").read()\n",
    "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "    display(\n",
    "        HTML(\n",
    "            \"\"\"\n",
    "          <video width=400 controls>\n",
    "                <source src=\"%s\" type=\"video/mp4\">\n",
    "          </video>\n",
    "      \"\"\"\n",
    "            % data_url\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddb2fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_video(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2aa926",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_video(output_vid_trk_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639f8215",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "95a22eaad15357b8581a3e85dc7deb44b392b83f2a0c98c7572069d11874febe"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

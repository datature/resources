{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.3.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (2.3.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorflow==2.3.0) (3.19.4)\n",
      "Collecting numpy<1.19.0,>=1.16.0\n",
      "  Downloading numpy-1.18.5-cp38-cp38-win_amd64.whl (12.8 MB)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorflow==2.3.0) (2.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorflow==2.3.0) (1.12.1)\n",
      "Requirement already satisfied: gast==0.3.3 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorflow==2.3.0) (0.3.3)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorflow==2.3.0) (1.46.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorflow==2.3.0) (2.3.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorflow==2.3.0) (1.6.3)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorflow==2.3.0) (0.37.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorflow==2.3.0) (3.3.0)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorflow==2.3.0) (2.10.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorflow==2.3.0) (1.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorflow==2.3.0) (0.2.0)\n",
      "Requirement already satisfied: scipy==1.4.1 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorflow==2.3.0) (1.4.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorflow==2.3.0) (1.16.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorflow==2.3.0) (1.0.0)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorflow==2.3.0) (1.1.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.3.7)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.25.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (56.0.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.8.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.1.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.6.6)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (5.1.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.11.4)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.8.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.26.9)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2022.5.18.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.10)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.2.0)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.22.4\n",
      "    Uninstalling numpy-1.22.4:\n",
      "      Successfully uninstalled numpy-1.22.4\n",
      "Successfully installed numpy-1.18.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script f2py.exe is installed in 'C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datature-hub 0.2.1 requires opencv-python==4.5.1.48, but you have opencv-python 4.5.5.64 which is incompatible.\n",
      "datature-hub 0.2.1 requires Pillow~=8.2, but you have pillow 9.1.1 which is incompatible.\n",
      "WARNING: You are using pip version 21.1.1; however, version 22.1.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\User\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.18.5 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (1.18.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 22.1.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\User\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PILLOW==7.2.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datature-hub 0.2.1 requires opencv-python==4.5.1.48, but you have opencv-python 4.5.5.64 which is incompatible.\n",
      "datature-hub 0.2.1 requires Pillow~=8.2, but you have pillow 7.2.0 which is incompatible.\n",
      "WARNING: You are using pip version 21.1.1; however, version 22.1.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\User\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached Pillow-7.2.0-cp38-cp38-win_amd64.whl (2.1 MB)\n",
      "Installing collected packages: PILLOW\n",
      "  Attempting uninstall: PILLOW\n",
      "    Found existing installation: Pillow 9.1.1\n",
      "    Uninstalling Pillow-9.1.1:\n",
      "      Successfully uninstalled Pillow-9.1.1\n",
      "Successfully installed PILLOW-7.2.0\n",
      "Collecting opencv-python==4.5.1.48\n",
      "  Using cached opencv_python-4.5.1.48-cp38-cp38-win_amd64.whl (34.9 MB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from opencv-python==4.5.1.48) (1.18.5)\n",
      "Installing collected packages: opencv-python\n",
      "  Attempting uninstall: opencv-python\n",
      "    Found existing installation: opencv-python 4.5.5.64\n",
      "    Uninstalling opencv-python-4.5.5.64:\n",
      "      Successfully uninstalled opencv-python-4.5.5.64\n",
      "Successfully installed opencv-python-4.5.1.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datature-hub 0.2.1 requires Pillow~=8.2, but you have pillow 7.2.0 which is incompatible.\n",
      "WARNING: You are using pip version 21.1.1; however, version 22.1.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\User\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf-slim in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (1.1.0)\n",
      "Requirement already satisfied: absl-py>=0.2.2 in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tf-slim) (1.0.0)\n",
      "Requirement already satisfied: six in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from absl-py>=0.2.2->tf-slim) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 22.1.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\User\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install -U tensorflow==2.3.0\n",
    "!pip install -U numpy==1.18.5\n",
    "!pip install -U PILLOW==7.2.0\n",
    "!pip install -U opencv-python==4.5.1.48\n",
    "!pip install -U scipy==1.4.1\n",
    "!pip install -U onnxruntime==1.12.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import glob\n",
    "import copy\n",
    "import argparse\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "\n",
    "from PIL import Image\n",
    "from scipy.special import expit\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" # Comment out to use GPU for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set all necessary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = \"./input\" # Path to folder that contains input images\n",
    "output_folder = \"./output\" # Path to folder to store predicted images\n",
    "width = 640 # Width of image to load into model\n",
    "height = 640 # Height of image to load into model\n",
    "threshold = 0.7 # Prediction confidence threshold\n",
    "model = \"./model.onnx\" # Path to exported tensorflow pb model\n",
    "label = \"./label.txt\" # Path to label map\n",
    "anchor = \"./anchors.txt\" # Path to anchor text file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load utilities functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anchors(anchors_path):\n",
    "    with open(anchors_path) as f:\n",
    "        anchors = f.readline()\n",
    "    anchors = [float(x) for x in anchors.split(\",\")]\n",
    "\n",
    "    return np.array(anchors).reshape(-1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_decode(\n",
    "    prediction,\n",
    "    anchors,\n",
    "    num_classes,\n",
    "    input_shape,\n",
    "):\n",
    "    \"\"\"Decode final layer features to bounding box parameters.\"\"\"\n",
    "    batch_size = np.shape(prediction)[0]\n",
    "    num_anchors = len(anchors)\n",
    "    grid_shape = np.shape(prediction)[1:3]\n",
    "\n",
    "    ## Check if stride on height & width are same\n",
    "    assert (input_shape[0] // grid_shape[0] == input_shape[1] //\n",
    "            grid_shape[1]), \"model stride mismatch.\"\n",
    "\n",
    "    prediction = np.reshape(\n",
    "        prediction,\n",
    "        (\n",
    "            batch_size,\n",
    "            grid_shape[0] * grid_shape[1] * num_anchors,\n",
    "            num_classes + 5,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    ## Generate x_y_offset grid map\n",
    "    grid_y = np.arange(grid_shape[0])\n",
    "    grid_x = np.arange(grid_shape[1])\n",
    "    x_offset, y_offset = np.meshgrid(grid_x, grid_y)\n",
    "\n",
    "    x_offset = np.reshape(x_offset, (-1, 1))\n",
    "    y_offset = np.reshape(y_offset, (-1, 1))\n",
    "\n",
    "    x_y_offset = np.concatenate((x_offset, y_offset), axis=1)\n",
    "    x_y_offset = np.tile(x_y_offset, (1, num_anchors))\n",
    "    x_y_offset = np.reshape(x_y_offset, (-1, 2))\n",
    "    x_y_offset = np.expand_dims(x_y_offset, 0)\n",
    "\n",
    "    ## Log space transform of the height and width\n",
    "    anchors = np.tile(anchors, (grid_shape[0] * grid_shape[1], 1))\n",
    "    anchors = np.expand_dims(anchors, 0)\n",
    "\n",
    "    ## Eliminate grid sensitivity\n",
    "    box_xy = (expit(prediction[..., :2]) +\n",
    "              x_y_offset) / np.array(grid_shape)[::-1]\n",
    "\n",
    "    box_wh = (np.exp(prediction[..., 2:4]) *\n",
    "              anchors) / np.array(input_shape)[::-1]\n",
    "\n",
    "    ## Sigmoid objectness scores\n",
    "    objectness = expit(prediction[..., 4])\n",
    "    objectness = np.expand_dims(objectness, -1)\n",
    "\n",
    "    ## Sigmoid class scores\n",
    "    class_scores = expit(prediction[..., 5:])\n",
    "\n",
    "    return np.concatenate([box_xy, box_wh, objectness, class_scores], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolov3v4_decode(\n",
    "    predictions,\n",
    "    anchors,\n",
    "    num_classes,\n",
    "    input_shape,\n",
    "):\n",
    "    \"\"\"\n",
    "    YOLOv3/v4 Head to process predictions from YOLOv3/v4 models\n",
    "\n",
    "    Args:\n",
    "        num_classes: Total number of classes\n",
    "        anchors: YOLO style anchor list for bounding box assignment\n",
    "        input_shape: Input shape of the image\n",
    "        predictions: A list of three tensors with shape (N, 19, 19, 255), (N, 38, 38, 255) and (N, 76, 76, 255)\n",
    "\n",
    "    Returns:\n",
    "        A tensor with the shape (N, num_boxes, 85)\n",
    "    \"\"\"\n",
    "    assert (len(predictions) == len(anchors) //\n",
    "            3), \"Anchor numbers does not match prediction.\"\n",
    "\n",
    "    if len(predictions) == 3:\n",
    "        anchor_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]\n",
    "    elif len(predictions) == 2:\n",
    "        anchor_mask = [[3, 4, 5], [0, 1, 2]]\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported prediction length: {}\".format(\n",
    "            len(predictions)))\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for idx, prediction in enumerate(predictions):\n",
    "        results.append(\n",
    "            yolo_decode(\n",
    "                prediction,\n",
    "                anchors[anchor_mask[idx]],\n",
    "                num_classes,\n",
    "                input_shape,\n",
    "            ))\n",
    "\n",
    "    return np.concatenate(results, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_correct_boxes(predictions, img_shape, model_input_shape):\n",
    "    box_xy = predictions[..., :2]\n",
    "    box_wh = predictions[..., 2:4]\n",
    "    objectness = np.expand_dims(predictions[..., 4], -1)\n",
    "    class_scores = predictions[..., 5:]\n",
    "\n",
    "    ## Model_input_shape & image_shape should be (height, width) format\n",
    "    model_input_shape = np.array(model_input_shape, dtype=\"float32\")\n",
    "    image_shape = np.array(img_shape, dtype=\"float32\")\n",
    "\n",
    "    new_shape = np.round(image_shape * np.min(model_input_shape / image_shape))\n",
    "    offset = (model_input_shape - new_shape) / 2.0 / model_input_shape\n",
    "    scale = model_input_shape / new_shape\n",
    "    ## Reverse offset/scale to match (w,h) order\n",
    "    offset = offset[..., ::-1]\n",
    "    scale = scale[..., ::-1]\n",
    "\n",
    "    box_xy = (box_xy - offset) * scale\n",
    "    box_wh *= scale\n",
    "\n",
    "    ## Convert centoids to top left coordinates\n",
    "    box_xy -= box_wh / 2\n",
    "\n",
    "    ## Scale boxes back to original image shape.\n",
    "    image_wh = image_shape[..., ::-1]\n",
    "    box_xy *= image_wh\n",
    "    box_wh *= image_wh\n",
    "\n",
    "    return np.concatenate([box_xy, box_wh, objectness, class_scores], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_diou(boxes):\n",
    "    ## Get box coordinate and area\n",
    "    x = boxes[:, 0]\n",
    "    y = boxes[:, 1]\n",
    "    w = boxes[:, 2]\n",
    "    h = boxes[:, 3]\n",
    "    areas = w * h\n",
    "\n",
    "    ## Check IoU\n",
    "    inter_xmin = np.maximum(x[1:], x[0])\n",
    "    inter_ymin = np.maximum(y[1:], y[0])\n",
    "    inter_xmax = np.minimum(x[1:] + w[1:], x[0] + w[0])\n",
    "    inter_ymax = np.minimum(y[1:] + h[1:], y[0] + h[0])\n",
    "\n",
    "    inter_w = np.maximum(0.0, inter_xmax - inter_xmin + 1)\n",
    "    inter_h = np.maximum(0.0, inter_ymax - inter_ymin + 1)\n",
    "\n",
    "    inter = inter_w * inter_h\n",
    "    iou = inter / (areas[1:] + areas[0] - inter)\n",
    "\n",
    "    ## Box center distance\n",
    "    x_center = x + w / 2\n",
    "    y_center = y + h / 2\n",
    "    center_distance = np.power(x_center[1:] - x_center[0], 2) + np.power(\n",
    "        y_center[1:] - y_center[0], 2)\n",
    "\n",
    "    ## Get enclosed area\n",
    "    enclose_xmin = np.minimum(x[1:], x[0])\n",
    "    enclose_ymin = np.minimum(y[1:], y[0])\n",
    "    enclose_xmax = np.maximum(x[1:] + w[1:], x[0] + w[0])\n",
    "    enclose_ymax = np.maximum(x[1:] + w[1:], x[0] + w[0])\n",
    "    enclose_w = np.maximum(0.0, enclose_xmax - enclose_xmin + 1)\n",
    "    enclose_h = np.maximum(0.0, enclose_ymax - enclose_ymin + 1)\n",
    "    ## Get enclosed diagonal distance\n",
    "    enclose_diagonal = np.power(enclose_w, 2) + np.power(enclose_h, 2)\n",
    "    ## Calculate DIoU, add epsilon in denominator to avoid dividing by 0\n",
    "    diou = iou - 1.0 * (center_distance) / (enclose_diagonal +\n",
    "                                            np.finfo(float).eps)\n",
    "\n",
    "    return diou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms_boxes(\n",
    "    boxes,\n",
    "    classes,\n",
    "    scores,\n",
    "    iou_threshold,\n",
    "):\n",
    "    nboxes, nclasses, nscores = [], [], []\n",
    "    for c in set(classes):\n",
    "        ## Handle data for one class\n",
    "        inds = np.where(classes == c)\n",
    "        b = boxes[inds]\n",
    "        c = classes[inds]\n",
    "        s = scores[inds]\n",
    "\n",
    "        ## Make a data copy to avoid breaking during nms operation\n",
    "        b_nms = copy.deepcopy(b)\n",
    "        c_nms = copy.deepcopy(c)\n",
    "        s_nms = copy.deepcopy(s)\n",
    "\n",
    "        while len(s_nms) > 0:\n",
    "            ## Store the box with the max score.\n",
    "            i = np.argmax(s_nms, axis=-1)\n",
    "            nboxes.append(copy.deepcopy(b_nms[i]))\n",
    "            nclasses.append(copy.deepcopy(c_nms[i]))\n",
    "            nscores.append(copy.deepcopy(s_nms[i]))\n",
    "\n",
    "            ## Swap the max line and first line\n",
    "            b_nms[[i, 0], :] = b_nms[[0, i], :]\n",
    "            c_nms[[i, 0]] = c_nms[[0, i]]\n",
    "            s_nms[[i, 0]] = s_nms[[0, i]]\n",
    "\n",
    "            iou = box_diou(b_nms)\n",
    "\n",
    "            ## Drop the last line since it has been record\n",
    "            b_nms = b_nms[1:]\n",
    "            c_nms = c_nms[1:]\n",
    "            s_nms = s_nms[1:]\n",
    "\n",
    "            keep_mask = np.where(iou <= iou_threshold)[0]\n",
    "\n",
    "            ## Keep needed box for next loop\n",
    "            b_nms = b_nms[keep_mask]\n",
    "            c_nms = c_nms[keep_mask]\n",
    "            s_nms = s_nms[keep_mask]\n",
    "\n",
    "    ## Reformat result for output\n",
    "    nboxes = [np.array(nboxes)]\n",
    "    nclasses = [np.array(nclasses)]\n",
    "    nscores = [np.array(nscores)]\n",
    "    return nboxes, nclasses, nscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_boxes(boxes, classes, scores, max_boxes):\n",
    "    ## Sort result according to scores\n",
    "    sorted_indices = np.argsort(scores)\n",
    "    sorted_indices = sorted_indices[::-1]\n",
    "    nboxes = boxes[sorted_indices]\n",
    "    nclasses = classes[sorted_indices]\n",
    "    nscores = scores[sorted_indices]\n",
    "\n",
    "    ## Only pick max_boxes\n",
    "    nboxes = nboxes[:max_boxes]\n",
    "    nclasses = nclasses[:max_boxes]\n",
    "    nscores = nscores[:max_boxes]\n",
    "\n",
    "    return nboxes, nclasses, nscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_handle_predictions(predictions,\n",
    "                            num_classes,\n",
    "                            max_boxes=100,\n",
    "                            confidence=0.1,\n",
    "                            iou_threshold=0.4):\n",
    "    \"\"\"Apply NMS algorithm & filter top max boxes.\"\"\"\n",
    "    boxes = predictions[:, :, :4]\n",
    "    box_confidences = np.expand_dims(predictions[:, :, 4], -1)\n",
    "    box_class_probs = predictions[:, :, 5:]\n",
    "\n",
    "    ## Check if only 1 class for different score\n",
    "    if num_classes == 1:\n",
    "        box_scores = box_confidences\n",
    "    else:\n",
    "        box_scores = box_confidences * box_class_probs\n",
    "\n",
    "    ## Filter boxes with score threshold\n",
    "    box_classes = np.argmax(box_scores, axis=-1)\n",
    "    box_class_scores = np.max(box_scores, axis=-1)\n",
    "    pos = np.where(box_class_scores >= float(confidence))\n",
    "\n",
    "    boxes = boxes[pos]\n",
    "    classes = box_classes[pos]\n",
    "    scores = box_class_scores[pos]\n",
    "\n",
    "    ## Boxes, Classes and Scores returned from NMS\n",
    "    n_boxes, n_classes, n_scores = nms_boxes(\n",
    "        boxes,\n",
    "        classes,\n",
    "        scores,\n",
    "        iou_threshold,\n",
    "    )\n",
    "\n",
    "    if n_boxes:\n",
    "        boxes = np.concatenate(n_boxes)\n",
    "        classes = np.concatenate(n_classes).astype(\"int32\")\n",
    "        scores = np.concatenate(n_scores)\n",
    "        boxes, classes, scores = filter_boxes(boxes, classes, scores,\n",
    "                                              max_boxes)\n",
    "\n",
    "        return boxes, classes, scores\n",
    "\n",
    "    return [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_adjust_boxes(boxes, img_shape):\n",
    "    \"\"\"Change box format from (x,y,w,h) top left coordinate to\n",
    "    (xmin,ymin,xmax,ymax) format\n",
    "    \"\"\"\n",
    "    if boxes is None or len(boxes) == 0:\n",
    "        return []\n",
    "\n",
    "    image_shape = np.array(img_shape, dtype=\"float32\")\n",
    "    height, width = image_shape\n",
    "\n",
    "    adjusted_boxes = []\n",
    "    for box in boxes:\n",
    "        x, y, w, h = box\n",
    "\n",
    "        xmin = min(max(0, x / width), 1)\n",
    "        ymin = min(max(0, y / height), 1)\n",
    "        xmax = min(max(xmin, (x + w) / width), 1)\n",
    "        ymax = min(max(ymin, (y + h) / height), 1)\n",
    "\n",
    "        adjusted_boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "    return np.array(adjusted_boxes, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolov3v4_postprocess(\n",
    "    yolo_outputs,\n",
    "    image_shape,\n",
    "    anchors,\n",
    "    num_classes,\n",
    "    model_input_shape,\n",
    "    max_boxes=100,\n",
    "    confidence=0.1,\n",
    "    iou_threshold=0.4,\n",
    "):\n",
    "    predictions = yolov3v4_decode(\n",
    "        yolo_outputs,\n",
    "        anchors,\n",
    "        num_classes,\n",
    "        input_shape=model_input_shape,\n",
    "    )\n",
    "\n",
    "    predictions = yolo_correct_boxes(predictions, image_shape,\n",
    "                                     model_input_shape)\n",
    "\n",
    "    boxes, classes, scores = yolo_handle_predictions(\n",
    "        predictions,\n",
    "        num_classes,\n",
    "        max_boxes=max_boxes,\n",
    "        confidence=confidence,\n",
    "        iou_threshold=iou_threshold,\n",
    "    )\n",
    "\n",
    "    boxes = yolo_adjust_boxes(boxes, image_shape)\n",
    "\n",
    "    return boxes, classes, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_label_map(label_map_path):\n",
    "    label_map = {}\n",
    "\n",
    "    with open(label_map_path, \"r\") as label_file:\n",
    "        for idx, line in enumerate(label_file):\n",
    "            tag_name = line.rstrip(\"\\n\")\n",
    "            label_map[idx] = {\n",
    "                \"id\": idx,\n",
    "                \"name\": tag_name,\n",
    "            }\n",
    "\n",
    "    return label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(path, width, height):\n",
    "    image = Image.open(path).convert(\"RGB\")\n",
    "    image_shape = np.asarray(image).shape\n",
    "\n",
    "    image_resized = image.resize((width, height))\n",
    "    return np.array(image_resized).astype(\"float32\"), (\n",
    "        image_shape[0],\n",
    "        image_shape[1],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load label_map, color_map and model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     color_map[each_class] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m256\u001b[39m), size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)]\n\u001b[0;32m      7\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 8\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39msaved_model\u001b[38;5;241m.\u001b[39mload(model)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel loaded, took \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m seconds...\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "category_index = load_label_map(label)\n",
    "\n",
    "color_map = {}\n",
    "for each_class in range(len(category_index)):\n",
    "    color_map[each_class] = [int(i) for i in np.random.choice(range(256), size=3)]\n",
    "\n",
    "anchors = get_anchors(anchor)\n",
    "    \n",
    "start_time = time.time()\n",
    "session = ort.InferenceSession(\n",
    "    args.model,\n",
    "    providers=[\"CUDAExecutionProvider\"],\n",
    ")\n",
    "\n",
    "print(\"Model loaded, took {} seconds...\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through images and obtain predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting for ./input\\361.jpg...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m height, width \u001b[38;5;241m=\u001b[39m size\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m## Returned original_shape is in the format of width, height\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m image_resized, origi_shape \u001b[38;5;241m=\u001b[39m \u001b[43mload_image_into_numpy_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43meach_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mheight\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m## The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(image_resized)\n",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36mload_image_into_numpy_array\u001b[1;34m(path, height, width)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_image_into_numpy_array\u001b[39m(path, height, width):\n\u001b[1;32m----> 2\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241m.\u001b[39mopen(path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m     image_shape \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(image)\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m      4\u001b[0m     image_resized \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mresize((height, width))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "## Run prediction on each image\n",
    "for each_image in glob.glob(os.path.join(args.input, \"*\")):\n",
    "    print(\"Predicting for {}...\".format(each_image))\n",
    "\n",
    "    ## Returned original_shape is in the format of width, height\n",
    "    image_resized, origi_shape = load_image_into_numpy_array(\n",
    "        each_image, int(height), int(width))\n",
    "\n",
    "    ## Normalize input image\n",
    "    input_image = (image_resized / 255.0).astype(np.float32)\n",
    "\n",
    "    detections_output = session.run(\n",
    "        output_names, {input_name: np.expand_dims(input_image, axis=0)})\n",
    "\n",
    "    bboxes, classes, scores = yolov3v4_postprocess(\n",
    "        detections_output, (int(width), int(height)),\n",
    "        anchors,\n",
    "        3, (int(width), int(height)),\n",
    "        confidence=args.threshold)\n",
    "\n",
    "    ## Draw Predictions\n",
    "    image_origi = np.array(\n",
    "        Image.fromarray(image_resized.astype(np.uint8)).resize(\n",
    "            (origi_shape[1], origi_shape[0])))\n",
    "\n",
    "    if len(bboxes) != 0:\n",
    "        for idx, each_bbox in enumerate(bboxes):\n",
    "\n",
    "            color = color_map.get(classes[idx] - 1)\n",
    "\n",
    "            ## Draw bounding box\n",
    "            cv2.rectangle(\n",
    "                image_origi,\n",
    "                (\n",
    "                    int(each_bbox[0] * origi_shape[1]),\n",
    "                    int(each_bbox[1] * origi_shape[0]),\n",
    "                ),\n",
    "                (\n",
    "                    int(each_bbox[2] * origi_shape[1]),\n",
    "                    int(each_bbox[3] * origi_shape[0]),\n",
    "                ),\n",
    "                color,\n",
    "                2,\n",
    "            )\n",
    "\n",
    "            # Draw label background\n",
    "            cv2.rectangle(\n",
    "                image_origi,\n",
    "                (\n",
    "                    int(each_bbox[0] * origi_shape[1]),\n",
    "                    int(each_bbox[3] * origi_shape[0]),\n",
    "                ),\n",
    "                (\n",
    "                    int(each_bbox[2] * origi_shape[1]),\n",
    "                    int(each_bbox[3] * origi_shape[0] + 15),\n",
    "                ),\n",
    "                color,\n",
    "                -1,\n",
    "            )\n",
    "\n",
    "            ## Insert label class & score\n",
    "            cv2.putText(\n",
    "                image_origi,\n",
    "                \"Class: {}, Score: {}\".format(\n",
    "                    str(category_index[classes[idx]][\"name\"]),\n",
    "                    str(round(scores[idx], 2)),\n",
    "                ),\n",
    "                (\n",
    "                    int(each_bbox[0] * origi_shape[1]),\n",
    "                    int(each_bbox[3] * origi_shape[0] + 10),\n",
    "                ),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.3,\n",
    "                (0, 0, 0),\n",
    "                1,\n",
    "                cv2.LINE_AA,\n",
    "            )\n",
    "\n",
    "        ## Save predicted image\n",
    "        filename = os.path.basename(each_image)\n",
    "        image_predict = Image.fromarray(image_origi)\n",
    "        image_predict.save(os.path.join(args.output, filename))\n",
    "\n",
    "        print(\"Saving predicted images to {}...\".format(\n",
    "            os.path.join(args.output, filename)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

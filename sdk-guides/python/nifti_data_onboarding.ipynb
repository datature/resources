{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*-coding:utf-8 -*-\n",
    "'''\n",
    "  ████\n",
    "██    ██   Datature\n",
    "  ██  ██   Powering Breakthrough AI\n",
    "    ██\n",
    "\n",
    "@File    :   nifti_data_onboarding.ipynb\n",
    "@Author  :   Trevor Carrell\n",
    "@Version :   2.0\n",
    "@Contact :   hello@datature.io\n",
    "@License :   Apache License 2.0\n",
    "@Desc    :   Demo for uploading NIfTI files using Datature Python SDK\n",
    "             and converting bitmask annotations to COCO polygon annotation\n",
    "             files.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a COCO formatted file using an RLE Mask for object annotation via `.nii` file uploading.\n",
    "In this python notebook, we will be using `.nii` files and their corresponding labels to create a COCO formatted file which allows Datature users to upload their `.nii` files and labels to create a COCO formatted file, which will annotate their uploaded files.\n",
    "\n",
    "## Uploading testing files to Nexus using Datature SDK:\n",
    "Before we start creating our COCO formatted file, we first need to upload our `.nii` files onto Nexus. To do so, we will utilize Datature's SDK, which converts our `.nii` files to `.mp4` files. Note that this conversion is necessary to allow for interpolation when using the annotation tool in Nexus.\n",
    "\n",
    "### Install / Import necessary libaries:\n",
    "In this python notebook, we will be using a few libraries to perform the task at hand. We do this below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle installation of packages.\n",
    "! pip3 install -U pip       # Upgrade pip.\n",
    "! pip3 install alive-progress  # Install alive-progress package.\n",
    "! pip3 install -U datature  # Install and update datature package.\n",
    "! pip3 install -U matplotlib       # Install matplotlib package.\n",
    "! pip3 install nibabel             # Install nibabel package.\n",
    "! pip3 install numpy               # Install numpy package.\n",
    "! pip3 install pycocotools         # Install pycocotools package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://asia-python.pkg.dev/datature-puppeteer/python/simple/\n",
      "Requirement already satisfied: keyring in /Users/trevorcarrell/anaconda3/envs/datature_series_p11/lib/python3.11/site-packages (24.3.0)\n",
      "Requirement already satisfied: jaraco.classes in /Users/trevorcarrell/anaconda3/envs/datature_series_p11/lib/python3.11/site-packages (from keyring) (3.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.11.4 in /Users/trevorcarrell/anaconda3/envs/datature_series_p11/lib/python3.11/site-packages (from keyring) (7.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/trevorcarrell/anaconda3/envs/datature_series_p11/lib/python3.11/site-packages (from importlib-metadata>=4.11.4->keyring) (3.17.0)\n",
      "Requirement already satisfied: more-itertools in /Users/trevorcarrell/anaconda3/envs/datature_series_p11/lib/python3.11/site-packages (from jaraco.classes->keyring) (10.2.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://asia-python.pkg.dev/datature-puppeteer/python/simple/\n",
      "Requirement already satisfied: keyrings.google-artifactregistry-auth in /Users/trevorcarrell/anaconda3/envs/datature_series_p11/lib/python3.11/site-packages (1.1.2)\n",
      "Requirement already satisfied: google-auth in /Users/trevorcarrell/anaconda3/envs/datature_series_p11/lib/python3.11/site-packages (from keyrings.google-artifactregistry-auth) (2.26.2)\n",
      "Requirement already satisfied: keyring in /Users/trevorcarrell/anaconda3/envs/datature_series_p11/lib/python3.11/site-packages (from keyrings.google-artifactregistry-auth) (24.3.0)\n",
      "Requirement already satisfied: pluggy in /Users/trevorcarrell/anaconda3/envs/datature_series_p11/lib/python3.11/site-packages (from keyrings.google-artifactregistry-auth) (1.3.0)\n",
      "Requirement already satisfied: requests in /Users/trevorcarrell/anaconda3/envs/datature_series_p11/lib/python3.11/site-packages (from keyrings.google-artifactregistry-auth) (2.28.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/trevorcarrell/anaconda3/envs/datature_series_p11/lib/python3.11/site-packages (from google-auth->keyrings.google-artifactregistry-auth) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/trevorcarrell/anaconda3/envs/datature_series_p11/lib/python3.11/site-packages (from google-auth->keyrings.google-artifactregistry-auth) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/trevorcarrell/anaconda3/envs/datature_series_p11/lib/python3.11/site-packages (from google-auth->keyrings.google-artifactregistry-auth) (4.9)\n",
      "Requirement already satisfied: jaraco.classes in /Users/trevorcarrell/anaconda3/envs/datature_series_p11/lib/python3.11/site-packages (from keyring->keyrings.google-artifactregistry-auth) (3.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.11.4 in /Users/trevorcarrell/anaconda3/envs/datature_series_p11/lib/python3.11/site-packages (from keyring->keyrings.google-artifactregistry-auth) (7.0.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/trevorcarrell/anaconda3/envs/datature_series_p11/lib/python3.11/site-packages (from requests->keyrings.google-artifactregistry-auth) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/trevorcarrell/anaconda3/envs/datature_series_p11/lib/python3.11/site-packages (from requests->keyrings.google-artifactregistry-auth) (3.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/trevorcarrell/anaconda3/envs/datature_series_p11/lib/python3.11/site-packages (from requests->keyrings.google-artifactregistry-auth) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/trevorcarrell/anaconda3/envs/datature_series_p11/lib/python3.11/site-packages (from requests->keyrings.google-artifactregistry-auth) (2023.11.17)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/trevorcarrell/anaconda3/envs/datature_series_p11/lib/python3.11/site-packages (from importlib-metadata>=4.11.4->keyring->keyrings.google-artifactregistry-auth) (3.17.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/trevorcarrell/anaconda3/envs/datature_series_p11/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth->keyrings.google-artifactregistry-auth) (0.5.1)\n",
      "Requirement already satisfied: more-itertools in /Users/trevorcarrell/anaconda3/envs/datature_series_p11/lib/python3.11/site-packages (from jaraco.classes->keyring->keyrings.google-artifactregistry-auth) (10.2.0)\n"
     ]
    }
   ],
   "source": [
    "# Handle imports.\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from alive_progress import alive_bar  # For uploading progress bar.\n",
    "from datature import nexus\n",
    "from pycocotools import mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define data paths, secret key, and project key:\n",
    "Now that we've imported the necessary libraries, we should define the path which will contain our `.nii` files and labels, the path that we want to output our COCO formatted file, and our projects secret key (which is necessary to use Datature's SDK).\n",
    "\n",
    "When using on your machine, replace `DATA_PATH` with the path to your `.nii` labels, the `OUTPUT_PATH` to where you want to output to go.\n",
    "\n",
    "We then need to define our secret key and project ID to connect to our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/Users/trevorcarrell/Documents/Datature.nosync/nii to COCO Format Code/Medical AI Series/Dataset/\"   # Path to data.\n",
    "OUTPUT_PATH = \"/Users/trevorcarrell/Documents/Datature.nosync/nii to COCO Format Code/Medical AI Series/output\"  # Path to output.\n",
    "\n",
    "# To see how to get our secret key and project key, please read the information below this cell, after \"Data sanity checks\"!\n",
    "SECRET_KEY = \"YOUR_SECRET_KEY\"\n",
    "PROJECT_KEY = \"YOUR_PROJECT_KEY\"\n",
    "\n",
    "# Don't modify PROJECT_NAME\n",
    "PROJECT_NAME = f\"proj_{PROJECT_KEY}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data sanity checks:\n",
    "Now, before we continue, it's always good practice to ensure our `.nii` files and their labels have the same dimensions, and that we have the same number of each (for one `.nii` file, we need a label file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = glob.glob(\"*.nii\", root_dir=f'{DATA_PATH}/t1gd')  # Get all data files.\n",
    "label_files = glob.glob(\"*.nii\", root_dir=f'{DATA_PATH}/labels/original')  # Get all label files.\n",
    "\n",
    "# Ensure that we have the same number of data files and label files, and ensure that these files exist.\n",
    "assert len(data_files) > 0 and len(label_files) > 0, \"Data and labels directories must not be empty.\"\n",
    "assert len(data_files) == len(label_files), \"Number of data files and label files must be equal.\"\n",
    "\n",
    "# Ensure that each data file has a label file. Our naming schema is that the label file is the same name\n",
    "# as the data file, the first character is a \"l\".\n",
    "for file in data_files:\n",
    "\n",
    "    # Create the label file name we expect.\n",
    "    label_filepath = os.path.join(f'{DATA_PATH}/labels/original', f'l{file[1:]}')\n",
    "\n",
    "    # Ensure that the label file exists.\n",
    "    assert os.path.exists(label_filepath), f\"Label file {label_filepath} does not exist.\"\n",
    "\n",
    "    # Load the label file and data file.\n",
    "    label_file = nib.load(label_filepath)\n",
    "    data_file = nib.load(os.path.join(f'{DATA_PATH}/t1gd', file))\n",
    "\n",
    "    # Ensure that the label file and data file have the same shape.\n",
    "    assert label_file.shape == data_file.shape, f\"Label file {label_file} does not match data file {data_file}.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating your project's secret key:\n",
    "\n",
    "The steps are as follows:\n",
    "\n",
    "1. Sign up for a free Datature account at https://www.datature.io\n",
    "2. Create a new project on Nexus\n",
    "3. Go to the Integrations page\n",
    "4. Choose `Generate New Secret` to get your Secret Key\n",
    "5. Follow this script to use Datature SDK to upload the NifTi files to Nexus\n",
    "\n",
    "For more information about Datature's Python SDK, see https://developers.datature.io/docs/python-sdk.\n",
    "\n",
    "### Uploading `.nii` files to Datature Nexus:\n",
    "Note that for `.nii` files, each file is a separate 3D volume.\n",
    "\n",
    "* If the axis of orientation is provided, the SDK will upload a series of 2D slices corresponding to the specified orientation. Thus, you will only see one asset on Nexus which contains the 2D slices.\n",
    "\n",
    "* If the axis of orientation is **not** provided, the SDK will upload a series of 2D slices for each orientation (x, y, and z). Thus, you will see three assets on Nexus which contain the 2D slices using the axial (z orientation), coronal (y orientation), and sagittal (x orientation) planes.\n",
    "\n",
    "With that, we can begin.\n",
    "\n",
    "### Uploading our dataset using Datature's SDK\n",
    "To use Datature's SDK, we need to first create an `UploadSession` class. Then we use `.add_path(path, nifti-orientation='z')` to specify the path we want to upload, as well as the orientation of our uploads (z in this case).\n",
    "\n",
    "**Note that we only want to upload our `.nii` files and not the labels.**\n",
    "\n",
    "***!!! Uploading may take awhile !!!*** – have waited at most three and a half minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing upload assets   |████████████████████████████████████████| 10/10 [100%] in 44.0s (0.23/s) \n",
      "Waiting server processing |████████████████████████████████████████| 1/1 [100%] in 3:10.7 (0.01/s) \n"
     ]
    }
   ],
   "source": [
    "# Get our project: (here endpopint is just used since we're in beta)\n",
    "project = nexus.Client(SECRET_KEY).get_project(PROJECT_NAME)\n",
    "\n",
    "# Create an upload session using the Datature API.\n",
    "upload_session = project.assets.create_upload_session(groups=[\"main\"], background=True)\n",
    "\n",
    "# Get the files we need to upload.\n",
    "files = glob.glob(f\"{DATA_PATH}/t1gd/*.nii\")\n",
    "\n",
    "# Now we upload the data and labels to the upload session, using a progress bar to show the progress.\n",
    "with alive_bar(len(files), title='Preparing upload assets', title_length=25) as progress_bar, upload_session as session:\n",
    "    for file in files:\n",
    "      session.add_path(file, nifti_orientation='z')\n",
    "      progress_bar()\n",
    "\n",
    "# Get the operation ID's so we can track the upload progress.\n",
    "operations = upload_session.get_operation_ids()\n",
    "with alive_bar(len(operations), title='Waiting server processing',title_length=25) as progress_bar:\n",
    "  for op in operations:\n",
    "    project.operations.wait_until_done(op)\n",
    "    progress_bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating COCO formatted file from `.nii` files and labels:\n",
    "Now that we have uploaded our files to Nexus, we want to create annotations for those files using a run-length encoding (RLE) binary mask. To do so, we use the names of the files we created to create our COCO formatted file. ***Ultimately, we do not need to process each 2D slice to get the names of the files, so we can simply make a list or structure format for our files that we uploaded to Nexus***\n",
    "\n",
    "### More about our `.nii` files and their processing:\n",
    "Note that each of our `.nii` files are `(240, 240, 155)`, where the orientation is along the z-axis, meaning that we have 155 slices and each slice is `(240, 240)`. When converting to `.mp4`, this means that we create a video which runs through 155 frames of size `240px, 240px`.\n",
    "\n",
    "For a given `.nii` file, i.e. `d_0001.nii`, that file was uploaded to Nexus as a `.mp4` file named `d_0001-z.mp4`. We then represent each frame of the video file as it's own `.jpg` file, which is referred to as `d_0001-z#frame=[x].jpg`, where `[x]` is replaced with a frame numbered 0 to 154, inclusive.\n",
    "\n",
    "Also note that the `.nii` label files are `.nii` files themselves, but only contain the `category_id`'s for our identifable classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating our COCO formatted file:\n",
    "Since our `.nii` files are stored in the Nexus, we can start building our COCO formatted file using the names of the files we uploaded with naming scheme above.\n",
    "\n",
    "There are a few considerations taken into account when making the COCO formatted file:\n",
    " \n",
    "* We assume the file format is similar to the format described here: https://developers.datature.io/docs/uploading-annotations#coco-annotator-polygons--masks\n",
    "\n",
    "* We use run-length encoding (RLE) binary masks for each `.nii` slice to create `n` separate masks for each slice, where `n` is the number of classes. In our case, we have `n = 3` classes: `non-enhancing tumor`, `enhancing tumor`, `edema`. The absence of a class indicates the brain region is unafflicted.\n",
    "\n",
    "Now, we create the COCO formatted file as a dictionary, then conver it to a `.json` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_IMAGES = 155  # Number of images per .nii file.\n",
    "\n",
    "\n",
    "def create_rle_mask(mask_data: np.ndarray[int]) -> dict:\n",
    "    \"\"\"\n",
    "    Given a binary mask, we create the RLE mask.\n",
    "\n",
    "    Input:\n",
    "        mask_data (ndarray): binary mask\n",
    "\n",
    "    Output:\n",
    "        rle_mask (ndarray): RLE binary mask\n",
    "    \"\"\"\n",
    "\n",
    "    # Since the RLE mask requires a fortran array, we need to encode the mask_data as a fortran array.\n",
    "    rle_data = mask.encode(np.asfortranarray(mask_data).astype(np.uint8))\n",
    "\n",
    "    # Now we create the RLE mask using the string encoding of the bytes.\n",
    "    rle_mask = {'counts': rle_data['counts'].decode('ascii'), 'size': rle_data['size']}\n",
    "\n",
    "    return rle_mask\n",
    "\n",
    "\n",
    "def create_licenses_entry() -> list[dict]:\n",
    "    return [{'id': 0,\n",
    "             'name': \"Unknown\",\n",
    "             'url': \"\"}]\n",
    "\n",
    "\n",
    "def create_info_entry() -> dict:\n",
    "    return {'description': 'Datature Created COCO Format Dataset',\n",
    "            'url': '',\n",
    "            'version': 1,\n",
    "            'year': datetime.now(timezone.utc).strftime('%Y'),\n",
    "            'contributor': 'Datature',\n",
    "            'date_created': datetime.now(timezone.utc).strftime('%Y-%m-%dT%H:%M:%S.%f%z')}\n",
    "\n",
    "\n",
    "def create_annotation_entry(curr_image_id: int, curr_annotation_id: int, category_id: int, rle_mask: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Given the current image id, current annotation id, category id, and rle mask, we create the annotation entry.\n",
    "\n",
    "    Input:\n",
    "        curr_image_id (int): current image id\n",
    "        curr_annotation_id (int): current annotation id\n",
    "        category_id (int): category id\n",
    "        rle_mask (dict): RLE binary mask where the keys are 'counts' and 'size'.\n",
    "\n",
    "    Output:\n",
    "        annotation_entry (dict): annotation entry\n",
    "    \"\"\"\n",
    "    return {'id': curr_annotation_id,\n",
    "            'image_id': curr_image_id,\n",
    "            'category_id': category_id,\n",
    "            'segmentation': rle_mask,\n",
    "            'area': 0,\n",
    "            'bbox': [0, 0, 0, 0],\n",
    "            'iscrowd': 1}\n",
    "\n",
    "\n",
    "def create_image_entry(filename: str, img_shape: tuple[int, int], curr_image_id: int) -> dict:\n",
    "    \"\"\"\n",
    "    Given the filename, image shape, and current image id, we create the image entry.\n",
    "\n",
    "    Input:\n",
    "        filename (string): filename of the image\n",
    "        img_data (tuple): image shape, which is a tuple of the image's width and height\n",
    "        curr_image_id (int): current image id\n",
    "\n",
    "    Output:\n",
    "        image_entry (dict): image entry\n",
    "    \"\"\"\n",
    "    return {'id': curr_image_id,\n",
    "            'width': img_shape[0],\n",
    "            'height': img_shape[1],\n",
    "            'file_name': filename,\n",
    "            'license': 0,\n",
    "            'date_captured': datetime.now(timezone.utc).strftime('%Y-%m-%dT%H:%M:%S.%f%z')}\n",
    "\n",
    "\n",
    "def create_coco_json(nii_labels_path: str, classes: dict[str, int]) -> dict:\n",
    "    \"\"\"\n",
    "    Given the original nii_path, which is a directory to .nii label files, and the output_path, which we stored our updated\n",
    "    binary masks, we create a COCO format json file.\n",
    "\n",
    "    Input:\n",
    "        nii_labels_path (string): path to directory containing .nii files\n",
    "        output_path (string): path to directory where you want to save the numpy arrays\n",
    "        classes (list): list of classes in the dataset\n",
    "\n",
    "    Output:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the items in the COCO json file.\n",
    "    info = create_info_entry()\n",
    "    images = []\n",
    "    categories = [{'supercategory': key, 'id': val, 'name': key} for key, val in classes.items()]\n",
    "    annotations = []\n",
    "    licenses = create_licenses_entry()\n",
    "\n",
    "    # Use to keep track of the image id.\n",
    "    curr_image_id = 0\n",
    "    curr_annotation_id = 0\n",
    "\n",
    "    # Now we create annotations from each image's binary mask (located in output directory); simutaneously, we create the RLEs\n",
    "    for file in glob.glob(f'*.nii', root_dir=nii_labels_path):  # We specify root here since we use the the filename later.\n",
    "\n",
    "        # Load in the .nii file, create ndarray of the data.\n",
    "        nii_mask = nib.load(os.path.join(nii_labels_path, f'{file}'))\n",
    "        nii_mask_data = nii_mask.get_fdata()\n",
    "\n",
    "        # Go through each frame in the .nii label file and create its RLE binary mask.\n",
    "        for i in range(nii_mask_data.shape[2]):\n",
    "\n",
    "            # Add the image entry in the images dictionary (remember to add the frame number to the filename and orientation).\n",
    "            images.append(create_image_entry(f'd{file[1:-4]}-z#frame={i}.jpg', nii_mask_data[:, :, i].shape, curr_image_id))\n",
    "\n",
    "            # Now create the rle binary mask for each of the classes (not background) in the current frame.\n",
    "            for _, val in classes.items():\n",
    "                rle_mask_data = create_rle_mask(np.where(nii_mask_data[:, :, i] == val, 1, 0))\n",
    "                annotations.append(create_annotation_entry(curr_image_id, curr_annotation_id, val, rle_mask_data))\n",
    "                curr_annotation_id += 1\n",
    "\n",
    "            curr_image_id += 1\n",
    "\n",
    "    return {'info': info,\n",
    "            'images': images,\n",
    "            'annotations': annotations,\n",
    "            'categories': categories,\n",
    "            'licenses': licenses}\n",
    "\n",
    "\n",
    "# Now we begin creating our COCO format json file using the labels and classes for each image. Note that this function is\n",
    "# specific to the dataset we are working with, so it will need to be modified for different datasets. To see this, not that we\n",
    "# explicitly name the image entry by assuming a specific naming convention for our .nii files:\n",
    "#\n",
    "#             images.append(create_image_entry(f'd{file[1:-4]}-z#frame={i}.jpg', nii_mask_data[:, :, i], curr_image_id))\n",
    "#\n",
    "coco_dict = create_coco_json(f'{DATA_PATH}/labels/original', {'edema': 1, 'non-enhancing tumor': 2,'enhancing tumor': 3})\n",
    "\n",
    "# Save the coco_dict as a json file.\n",
    "with open(os.path.join(f'{OUTPUT_PATH}', 'coco.json'), 'w') as f:\n",
    "    json.dump(coco_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Culmination\n",
    "Ultimately, after running the last cell, we should have a COCO formatted `.json` file named `coco.json` in our `OUTPUT_PATH` directory, which can be uploaded using Nexus' Upload / Export Annotations feature. \n",
    "\n",
    "Here, when uploading, remember to specify that we are importing a file of format `[Polygon / Mask] COCO Mask` since we used an RLE binary mask."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
